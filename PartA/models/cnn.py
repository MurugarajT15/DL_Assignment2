# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CauD7nfwC07F10mnSJr1XBft6uVpl0VT
"""

import numpy as np
import pandas as pd
import os
import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torch.utils.data import random_split

class Convolutional_Neural_Network(nn.Module):
    def __init__(self, trainset, valset, batch_size , num_filters,filter_org, kernel_size,
                 act_fn, num_neurons, batch_norm, dropout_rate):
            super().__init__()

            # DataLoader for the dataset
            self.dataloader_train = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle=True)
            self.dataloader_val = torch.utils.data.DataLoader(valset, batch_size = batch_size, shuffle=True)

            self.batch_norm = batch_norm
            filters = []
            if filter_org=='same':
                filters = [num_filters]*5
            elif filter_org=='double':
                filters = [num_filters, num_filters*2, num_filters*4, num_filters*8, num_filters*16]
            elif filter_org=='half':
                filters = [num_filters, num_filters//2, num_filters//4, num_filters//8, num_filters//16]
        #Convolution layer 1
            self.conv_1 = nn.Conv2d(3, filters[0],
                               kernel_size=(kernel_size[0],kernel_size[0]),
                               stride=1, padding='same')
            x = 224
            self.bn_1 = nn.BatchNorm2d(filters[0])
            self.act_1 = self.activation_function(act_fn)
            self.dropout_1 = nn.Dropout(p=dropout_rate)
            self.pool_1 = nn.MaxPool2d(2,2)

            self.conv_2 = nn.Conv2d(filters[0], filters[1],
                               kernel_size=(kernel_size[1],kernel_size[1]),
                               stride=1, padding='same')
            self.bn_2 = nn.BatchNorm2d(filters[1])
            self.act_2 = self.activation_function(act_fn)
            self.dropout_2 = nn.Dropout(p=dropout_rate)
            self.pool_2 = nn.MaxPool2d(2,2)


        #Convolution layer 3
            self.conv_3 = nn.Conv2d(filters[1], filters[2],
                               kernel_size=(kernel_size[2],kernel_size[2]),
                               stride=1, padding='same')

            self.bn_3 = nn.BatchNorm2d(filters[2])
            self.act_3 = self.activation_function(act_fn)
            self.dropout_3 = nn.Dropout(p=dropout_rate)
            self.pool_3 = nn.MaxPool2d(2,2)


        #Convolution layer 4
            self.conv_4 = nn.Conv2d(filters[2], filters[3],
                               kernel_size=(kernel_size[3],kernel_size[3]),
                               stride=1, padding='same')
            self.bn_4 = nn.BatchNorm2d(filters[3])
            self.act_4 = self.activation_function(act_fn)
            self.dropout_4 = nn.Dropout(p=dropout_rate)
            self.pool_4 = nn.MaxPool2d(2,2)


        #Convolution layer 5
            self.conv_5 = nn.Conv2d(filters[3], filters[4],
                               kernel_size=(kernel_size[4],kernel_size[4]),
                               stride=1, padding='same')
            self.bn_5 = nn.BatchNorm2d(filters[4])
            self.act_5 = self.activation_function(act_fn)
            self.dropout_5 = nn.Dropout(p=dropout_rate)
            self.pool_5 = nn.MaxPool2d(2,2)
            x = x//(2**5)
            x = x*x*filters[4]

            self.flatten = nn.Flatten()

        # dense layer
            self.fc_6 = nn.Linear(x, num_neurons)
            self.act_6 = self.activation_function(act_fn)
            self.dropout_6 = nn.Dropout(p=dropout_rate)

        # output layer
            self.fc_7 = nn.Linear(num_neurons, 10)

    def activation_function(self,act_fn):
            if act_fn=='relu':
              return nn.ReLU()
            elif act_fn=='tanh':
              return nn.Tanh()
            elif act_fn=='gelu':
              return nn.GELU()
            elif act_fn=='silu':
              return nn.SiLU()
            elif act_fn=='mish':
              return nn.Mish()

    def forward(self, x):

        x = self.conv_1(x)
        if self.batch_norm:
            x = self.bn_1(x)
        x = self.act_1(x)
        x = self.dropout_1(x)
        x = self.pool_1(x)


        x = self.conv_2(x)
        if self.batch_norm:
            x = self.bn_2(x)
        x = self.act_2(x)
        x = self.dropout_2(x)

        x = self.pool_2(x)


        x = self.conv_3(x)
        if self.batch_norm:
            x = self.bn_3(x)
        x = self.act_3(x)
        x = self.dropout_3(x)
        x = self.pool_3(x)


        x = self.conv_4(x)
        if self.batch_norm:
            x = self.bn_4(x)
        x = self.act_4(x)
        x = self.dropout_4(x)

        x = self.pool_4(x)

        x = self.conv_5(x)
        if self.batch_norm:
            x = self.bn_5(x)
        x = self.act_5(x)
        x = self.dropout_5(x)
        x = self.pool_5(x)


        x = self.flatten(x)
        x = self.act_6(self.fc_6(x))
        x = self.dropout_6(x)
        x = self.fc_7(x)
        return x